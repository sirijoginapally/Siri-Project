{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPbVVWEMI_9k"
   },
   "source": [
    "## In this file, we are analysisng the tweets from Twitter related to EWR airport to find out the public sentimnet about delays at EWR airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X99SE7QQF9Gz",
    "outputId": "d1fab8ad-bb6e-4408-8533-f7dec8e91fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in /Users/sirismitharao/opt/anaconda3/lib/python3.9/site-packages (0.7.0.20230622)\n",
      "Requirement already satisfied: requests[socks] in /Users/sirismitharao/opt/anaconda3/lib/python3.9/site-packages (from snscrape) (2.28.1)\n",
      "Requirement already satisfied: lxml in /Users/sirismitharao/opt/anaconda3/lib/python3.9/site-packages (from snscrape) (4.9.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/sirismitharao/opt/anaconda3/lib/python3.9/site-packages (from snscrape) (4.11.1)\n",
      "Requirement already satisfied: filelock in /Users/sirismitharao/opt/anaconda3/lib/python3.9/site-packages (from snscrape) (3.9.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/sirismitharao/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->snscrape) (2.3.2.post1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/sirismitharao/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]->snscrape) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sirismitharao/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]->snscrape) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sirismitharao/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]->snscrape) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sirismitharao/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]->snscrape) (2022.12.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/sirismitharao/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# installing snscrape library to retrieve tweets from Twittter\n",
    "\n",
    "!pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "y2YjpEjyFmwx"
   },
   "outputs": [],
   "source": [
    "# importing necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iFg7J8V4GE5g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22%23%28%20EWR%20flight%20cancellations%20OR%20EWR%20customer%20service%29%20until%3A2022-12-30%20since%3A2022-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D: blocked (404)\n",
      "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22%23%28%20EWR%20flight%20cancellations%20OR%20EWR%20customer%20service%29%20until%3A2022-12-30%20since%3A2022-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.\n",
      "Errors: blocked (404), blocked (404), blocked (404), blocked (404)\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22%23%28%20EWR%20flight%20cancellations%20OR%20EWR%20customer%20service%29%20until%3A2022-12-30%20since%3A2022-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m tweets\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     10\u001b[0m limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m sntwitter\u001b[38;5;241m.\u001b[39mTwitterHashtagScraper(query1)\u001b[38;5;241m.\u001b[39mget_items():\n\u001b[1;32m     13\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tweets)\u001b[38;5;241m==\u001b[39mlimit:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/snscrape/modules/twitter.py:1763\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: variables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[1;32m   1761\u001b[0m paginationParams \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: paginationVariables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[0;32m-> 1763\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL, params, paginationParams, cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor, instructionsPath \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m   1764\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graphql_timeline_instructions_to_tweets(obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/snscrape/modules/twitter.py:915\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[0;34m(self, endpoint, apiType, params, paginationParams, cursor, direction, instructionsPath)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 915\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    918\u001b[0m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/snscrape/modules/twitter.py:886\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[0;34m(self, endpoint, apiType, params, instructionsPath)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL:\n\u001b[1;32m    885\u001b[0m \tparams \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlencode({k: json\u001b[38;5;241m.\u001b[39mdumps(v, separators \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()}, quote_via \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote)\n\u001b[0;32m--> 886\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apiHeaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_api_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39m_snscrapeObj\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/snscrape/base.py:275\u001b[0m, in \u001b[0;36mScraper._get\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 275\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/snscrape/base.py:271\u001b[0m, in \u001b[0;36mScraper._request\u001b[0;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[1;32m    269\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[1;32m    270\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 271\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mScraperException\u001b[0m: 4 requests to https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline?variables=%7B%22rawQuery%22%3A%22%23%28%20EWR%20flight%20cancellations%20OR%20EWR%20customer%20service%29%20until%3A2022-12-30%20since%3A2022-01-01%22%2C%22count%22%3A20%2C%22product%22%3A%22Latest%22%2C%22withDownvotePerspective%22%3Afalse%2C%22withReactionsMetadata%22%3Afalse%2C%22withReactionsPerspective%22%3Afalse%7D&features=%7B%22rweb_lists_timeline_redesign_enabled%22%3Afalse%2C%22blue_business_profile_image_shape_enabled%22%3Afalse%2C%22responsive_web_graphql_exclude_directive_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Afalse%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22tweetypie_unmention_optimization_enabled%22%3Atrue%2C%22vibe_api_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Afalse%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Afalse%2C%22interactive_text_enabled%22%3Atrue%2C%22responsive_web_text_conversations_enabled%22%3Afalse%2C%22longform_notetweets_rich_text_read_enabled%22%3Afalse%2C%22longform_notetweets_inline_media_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%2C%22responsive_web_twitter_blue_verified_badge_is_enabled%22%3Atrue%7D failed, giving up."
     ]
    }
   ],
   "source": [
    "# Getting tweets from Twitter using following key words\n",
    "\n",
    "query1=\"( EWR flight cancellations OR EWR customer service) until:2022-12-30 since:2022-01-01\"\n",
    "query2=\"( EWR security checkpoints OR EWR baggage claims) until:2022-12-30 since:2022-01-01\"\n",
    "query3=\"( Newark Liberty International Airport OR EWR) until:2022-12-30 since:2022-01-01\"\n",
    "query4=\"( EWR delay OR EWR over crowding) until:2022-12-30 since:2022-01-01\"\n",
    "query5=\"( EWR long wait times OR EWR weather) until:2022-12-30 since:2022-01-01\"\n",
    "query6=\"( EWR traffic control issue) until:2022-12-30 since:2022-01-01\"\n",
    "tweets=[]\n",
    "limit=10000\n",
    "\n",
    "for tweet in sntwitter.TwitterHashtagScraper(query1).get_items():\n",
    "  if len(tweets)==limit:\n",
    "    break\n",
    "  else:\n",
    "    tweets.append([tweet.date, tweet.content])\n",
    "\n",
    "for tweet in sntwitter.TwitterHashtagScraper(query2).get_items():\n",
    "  if len(tweets)==limit:\n",
    "    break\n",
    "  else:\n",
    "    tweets.append([tweet.date, tweet.content])\n",
    "\n",
    "for tweet in sntwitter.TwitterHashtagScraper(query3).get_items():\n",
    "  if len(tweets)==limit:\n",
    "    break\n",
    "  else:\n",
    "    tweets.append([tweet.date, tweet.content])\n",
    "\n",
    "for tweet in sntwitter.TwitterHashtagScraper(query4).get_items():\n",
    "  if len(tweets)==limit:\n",
    "    break\n",
    "  else:\n",
    "    tweets.append([tweet.date, tweet.content])\n",
    "\n",
    "for tweet in sntwitter.TwitterHashtagScraper(query5).get_items():\n",
    "  if len(tweets)==limit:\n",
    "    break\n",
    "  else:\n",
    "    tweets.append([tweet.date, tweet.content])\n",
    "\n",
    "for tweet in sntwitter.TwitterHashtagScraper(query6).get_items():\n",
    "  if len(tweets)==limit:\n",
    "    break\n",
    "  else:\n",
    "    tweets.append([tweet.date, tweet.content])\n",
    "\n",
    "# converting the tweets into a dataframe and then saving it as a .csv file\n",
    "\n",
    "df=pd.DataFrame(tweets, columns=['date','tweet'])\n",
    "df.to_csv('tweetsall.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsjuBMhDFtC5"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "txYDuAXAFtAF",
    "outputId": "e16228a2-9154-4b9e-df7b-9c348a461edc"
   },
   "outputs": [],
   "source": [
    "# creating dataframe named 'data' with all the tweets\n",
    " \n",
    "data=pd.read_csv('tweets1.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hehXlTILFs-H",
    "outputId": "33982b83-30e5-4308-c838-66a3f878ea56"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ExpVde7DFs66",
    "outputId": "d41cd17f-2b99-47ce-9c2e-2c0b5f2be4a4"
   },
   "outputs": [],
   "source": [
    "# displaying sample tweet\n",
    "\n",
    "data['tweet'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rVEBWxdFs4E"
   },
   "outputs": [],
   "source": [
    "# importing necessary modules\n",
    "\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHromnunFs0E",
    "outputId": "9da233d1-8ac6-4cc7-b348-658006db35b7"
   },
   "outputs": [],
   "source": [
    "# importing nltk module for tweets pre-processing\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lC7AmNUFsxb"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the tweets\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Converting to lowercase\n",
    "    text = text.lower()\n",
    "    # Removing URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Removing URLs\n",
    "    text = re.sub(r'https', '', text)\n",
    "    # Removing mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    # Removing punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenizing text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Removing stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lemmatizing tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Joining tokens into text\n",
    "    text = ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkF1a0zeFsvE"
   },
   "outputs": [],
   "source": [
    "# Creating a list of preprocessed tweets\n",
    "preprocessed_tweets = []\n",
    "for twt in data['tweet']:\n",
    "    preprocessed_tweet = preprocess_text(twt)\n",
    "    preprocessed_tweets.append(preprocessed_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ZZvZ8zlNFsp6",
    "outputId": "f89f8063-e540-4cce-a92b-5e7631aa1c62"
   },
   "outputs": [],
   "source": [
    "#printing the first 10 preprocessed clean tweets\n",
    "#we can see that #hashtag and url is removed\n",
    "preprocessed_tweets[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYSocJ71FsnL"
   },
   "outputs": [],
   "source": [
    "# Performing sentiment analysis on the preprocessed tweets\n",
    "sentiments = []\n",
    "positive_tweets = 0\n",
    "negative_tweets = 0\n",
    "neutral_tweets = 0\n",
    "i=0\n",
    "for twt in preprocessed_tweets:\n",
    "    blob = TextBlob(twt)\n",
    "    sentiment_score = blob.sentiment.polarity\n",
    "    data.loc[i, 'sentiment score']=sentiment_score\n",
    "    i=i+1\n",
    "    if sentiment_score > 0:\n",
    "        sentiment = 'positive'\n",
    "        positive_tweets += 1\n",
    "    elif sentiment_score < 0:\n",
    "        sentiment = 'negative'\n",
    "        negative_tweets += 1\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "        neutral_tweets += 1\n",
    "    sentiments.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KftfK0hFsiS",
    "outputId": "13c00a75-9ca2-417f-87a7-aabc418bf8f2"
   },
   "outputs": [],
   "source": [
    "#printing the sentiments of the preprocessed tweets\n",
    "sentiments[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alnrZ-UAJNh7"
   },
   "source": [
    "## Sentiment Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "6d-pqio_Fsfi",
    "outputId": "e7673768-41e9-4d8f-8855-11400a02714e"
   },
   "outputs": [],
   "source": [
    "# Creating a pie chart to visualize sentiment distribution of tweets on EWR airport\n",
    "\n",
    "labels = ['Positive', 'Negative', 'Neutral']\n",
    "sizes = [positive_tweets, negative_tweets, neutral_tweets]\n",
    "colors = ['yellowgreen', 'lightcoral', 'gold']\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title('Sentiment Analysis of Tweets on EWR Airport Delays')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBi9T57-FsdK"
   },
   "outputs": [],
   "source": [
    "# adding 'sentimnet' column to dataframe\n",
    "\n",
    "data['sentiment']=sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qu8kY8hCPqOb"
   },
   "outputs": [],
   "source": [
    "# adding 'processed tweet' column to dataframe\n",
    "\n",
    "data['processed tweet']=preprocessed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "uHWkapeLFsac",
    "outputId": "d8ee2930-6eb8-4784-c59b-bce89a447551"
   },
   "outputs": [],
   "source": [
    "# displaying first five rows of dataframe\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "AmtH3I_hLPUz",
    "outputId": "eb5a85e4-ade0-4000-c090-0dca50607b80"
   },
   "outputs": [],
   "source": [
    "# displaying last five rows of datafrem\n",
    "\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-hJ8CcLJR_9"
   },
   "source": [
    "## Word Cloud of processed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "J0LnS68eYvd8",
    "outputId": "a2436b1e-e39b-452d-b094-4190d88e446e"
   },
   "outputs": [],
   "source": [
    "# Plotting The Word Cloud of processed tweets\n",
    "\n",
    "allWords = ' '.join( [twts for twts in data['processed tweet']])\n",
    "\n",
    "wordcloud = WordCloud(width = 500, height=300, random_state = 21, max_font_size = 119, background_color='white'). generate(allWords)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation = \"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5BTlqAWD-jK"
   },
   "source": [
    "## USING GRIDSEARCH CV FOR LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjQjGJQOEESZ"
   },
   "outputs": [],
   "source": [
    "# importing necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6Nb7SAzEKOC"
   },
   "outputs": [],
   "source": [
    "#using the CountVectorizer to convert the text into a matrix of token counts:\n",
    "vectorizer2 = CountVectorizer(stop_words='english')\n",
    "X = vectorizer2.fit_transform(data['processed tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RaLh60yOE4oJ"
   },
   "outputs": [],
   "source": [
    "#Defining the LDA model and the parameter grid for GridSearchCV:\n",
    "\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "param_grid = {\n",
    "    'n_components': [5, 10, 15],\n",
    "    'learning_decay': [.5, .7, .9],\n",
    "    'max_iter': [10, 50, 100]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "pnYint_OE9Cy",
    "outputId": "ec6f8d5c-5639-4584-9a4e-bea6a9bf5899"
   },
   "outputs": [],
   "source": [
    "# Using GridSearchCV to search for the best hyperparameters:\n",
    "grid_search = GridSearchCV(lda, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLyoBK2NFAfD",
    "outputId": "c92cdc9c-062a-4f5e-d327-f7f8ffe5d591"
   },
   "outputs": [],
   "source": [
    "# Printing the best hyperparameters and the corresponding score:\n",
    "print('Best Hyperparameters: ', grid_search.best_params_)\n",
    "print('Best Score: ', grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yqfv0QO9JgxS"
   },
   "source": [
    "## Getting best parameters for LDA using GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "8tk7O-gPYCdR",
    "outputId": "77989810-ddcf-4603-f286-9593ee72b88b"
   },
   "outputs": [],
   "source": [
    "# Defining the number of topics and perform topic modeling using LDA\n",
    "num_topics = 5\n",
    "lda_model2 = LatentDirichletAllocation(n_components=num_topics, max_iter=100, learning_method='online', random_state=42,learning_decay=0.5)\n",
    "lda_model2.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWKowllAJj5q"
   },
   "source": [
    "## Topics modelled by LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFCibEKdY6Kp",
    "outputId": "eeb6a707-1a72-4dd0-f4c0-1d5dab5c8377"
   },
   "outputs": [],
   "source": [
    "# Printing the top 10 terms for each topic\n",
    "for idx, topic in enumerate(lda_model2.components_):\n",
    "    print('Topic: ', idx)\n",
    "    print(' '.join([vectorizer2.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvyUJVc8r6pt",
    "outputId": "68615915-74b7-40f3-b1f8-72e9d8c5efc6"
   },
   "outputs": [],
   "source": [
    "# printing the number of tweets topic wise and we can see that around 750 tweets are about Topic-2.\n",
    "\n",
    "from collections import Counter\n",
    "topic_counts = Counter(lda_model2.transform(X).argmax(axis=1))\n",
    "\n",
    "for topic_num, count in sorted(topic_counts.items()):\n",
    "    print(f\"Topic {topic_num}: {count} tweets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lBbxHTQwpwUu",
    "outputId": "7a788541-720b-4920-e9e9-d90f6a98ed06"
   },
   "outputs": [],
   "source": [
    "# finding out the sentiment score of each topic to find out whether that particular topic is Positive, Neagtive or Neutral. We can see that all the topics are Negative\n",
    "\n",
    "tweets_by_topic = {}\n",
    "for i, topic in enumerate(lda_model2.components_):\n",
    "    topic_tweets = []\n",
    "    for j in range(len(data)):\n",
    "        topic_probs = lda_model2.transform(X[j])\n",
    "        topic_index = np.argmax(topic_probs)\n",
    "        if topic_index == i:\n",
    "            topic_tweets.append(data['processed tweet'][j])\n",
    "    tweets_by_topic[i] = topic_tweets\n",
    "for topic_num, topic_tweets in tweets_by_topic.items():\n",
    "    polarity = 0\n",
    "    for tweet in topic_tweets:\n",
    "        blob = TextBlob(tweet)\n",
    "        polarity += blob.sentiment.polarity\n",
    "    avg_polarity = polarity / len(topic_tweets)\n",
    "    print(f\"Topic {topic_num} sentiment: {avg_polarity}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
